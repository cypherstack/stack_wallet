/* 
 * This file is part of Stack Wallet.
 * 
 * Copyright (c) 2025 Cypher Stack
 * All Rights Reserved.
 * The code is distributed under GPLv3 license, see LICENSE file for details.
 * Generated by Cypher Stack on 2025-08-14
 *
 */

import 'dart:async';
import 'dart:typed_data';
import 'package:flutter/material.dart';
import 'package:logger/logger.dart';
import 'package:flutter_libsparkmobile/flutter_libsparkmobile.dart' as lib_spark;
import '../../../utilities/logger.dart';
import '../test_suite_interface.dart';
import '../testing_models.dart';

class FiroIntegrationTestSuite implements TestSuiteInterface {
  final StreamController<TestSuiteStatus> _statusController = 
      StreamController<TestSuiteStatus>.broadcast();
  TestSuiteStatus _status = TestSuiteStatus.waiting;

  @override
  String get displayName => "Firo Integration";

  @override
  Widget get icon => const Icon(Icons.local_fire_department, size: 32);

  @override
  TestSuiteStatus get status => _status;

  @override
  Stream<TestSuiteStatus> get statusStream => _statusController.stream;

  @override
  Future<TestResult> runTests() async {
    final stopwatch = Stopwatch()..start();
    
    try {
      _updateStatus(TestSuiteStatus.running);
      
      Logging.instance.log(Level.info, "Starting Firo integration test suite...");

      await _testLibSparkBasicIntegration();
      await _testSparkAddressGeneration();
      // await _testSparkCoinIdentification();
      
      stopwatch.stop();
      _updateStatus(TestSuiteStatus.passed);
      
      return TestResult(
        success: true,
        message: "üëçüëç All Firo integration tests passed successfully",
        executionTime: stopwatch.elapsed,
      );
      
    } catch (e, stackTrace) {
      stopwatch.stop();
      _updateStatus(TestSuiteStatus.failed);
      
      Logging.instance.log(Level.error, 
        "Firo integration test suite failed: $e\n$stackTrace"
      );
      
      return TestResult(
        success: false,
        message: "Firo integration tests failed: $e",
        executionTime: stopwatch.elapsed,
      );
    }
  }

  Future<void> _testLibSparkBasicIntegration() async {
    Logging.instance.log(Level.info, "Testing LibSpark basic integration...");
    
    try {
      // Test basic LibSpark function calls to ensure FFI integration is working.
      
      // Test address validation - this should work without generating keys.
      const validTestnetAddress = "st13nzr56g59tuaj2fs7xcy8hk98cv8ten4u64qzevv98tyt6mku5stnu6rtkan448g4erz0a85xjwjqdhf0xnxltymva68rmhr50smn0vyyluhflyzxx2f2x0u2ea8fq7zh2an9zc7g6lrj";
      const invalidAddress = "invalid_spark_address";
      
      // Test address validation for testnet.
      final isValidTestnet = lib_spark.LibSpark.validateAddress(
        address: validTestnetAddress,
        isTestNet: true,
      );
      
      if (!isValidTestnet) {
        throw Exception("Valid testnet Spark address was marked as invalid");
      }
      
      // Test invalid address.
      final isInvalid = lib_spark.LibSpark.validateAddress(
        address: invalidAddress,
        isTestNet: true,
      );
      
      if (isInvalid) {
        throw Exception("Invalid Spark address was marked as valid");
      }
      
      Logging.instance.log(Level.info, 
        "üëç LibSpark basic integration test passed"
      );
      
    } catch (e) {
      throw Exception("LibSpark basic integration test failed: $e");
    }
  }

  Future<void> _testSparkAddressGeneration() async {
    Logging.instance.log(Level.info, "Testing Spark address generation...");
    
    try {
      // Generate test private key (32 bytes).
      final testPrivateKey = Uint8List.fromList(
        List.generate(32, (index) => index + 1)
      );
      
      // Test address generation for testnet.
      final sparkAddress = await lib_spark.LibSpark.getAddress(
        privateKey: testPrivateKey,
        index: 1,
        diversifier: 1,
        isTestNet: true,
      );
      
      // Validate generated address.
      if (sparkAddress.isEmpty) {
        throw Exception("Generated Spark address is empty");
      }
      
      // Verify the generated address is valid.
      final isValid = lib_spark.LibSpark.validateAddress(
        address: sparkAddress,
        isTestNet: true,
      );
      
      if (!isValid) {
        throw Exception("Generated Spark address is invalid: $sparkAddress");
      }
      
      // Test address generation with different diversifier.
      final sparkAddress2 = await lib_spark.LibSpark.getAddress(
        privateKey: testPrivateKey,
        index: 1,
        diversifier: 2,
        isTestNet: true,
      );
      
      if (sparkAddress2.isEmpty) {
        throw Exception("Generated Spark address with diversifier 2 is empty");
      }
      
      // Addresses with different diversifiers should be different.
      if (sparkAddress == sparkAddress2) {
        throw Exception("Addresses with different diversifiers should be different");
      }
      
      Logging.instance.log(Level.info, 
        "üëç Spark address generation test passed: $sparkAddress"
      );
      
    } catch (e) {
      throw Exception("Spark address generation test failed: $e");
    }
  }

  Future<void> _testSparkCoinIdentification() async {
    Logging.instance.log(Level.info, "Testing Spark coin identification...");
    
    try {
      // Test with dummy data to ensure the identify function can be called.
      // This tests the FFI binding is working correctly.
      final testPrivateKey = Uint8List.fromList(
        List.generate(32, (index) => index + 1)
      );
      
      // Create dummy serialized coin data (base64 encoded dummy data).
      final dummySerializedCoin = "dGVzdERhdGE="; // "testData" in base64.
      final dummyContext = Uint8List.fromList([1, 2, 3, 4]);
      
      // This should return null for dummy data but shouldn't crash.
      final identifiedCoin = lib_spark.LibSpark.identifyAndRecoverCoin(
        dummySerializedCoin,
        privateKeyHex: testPrivateKey.map((b) => b.toRadixString(16).padLeft(2, '0')).join(),
        index: 1,
        context: dummyContext,
        isTestNet: true,
      );
      
      // We expect null for dummy data, which is correct behavior.
      if (identifiedCoin != null) {
        Logging.instance.log(Level.info, 
          "Coin identification returned non-null for dummy data (unexpected but not necessarily an error)"
        );
      }
      
      Logging.instance.log(Level.info, 
        "üëç Spark coin identification test passed"
      );
      
    } catch (e) {
      throw Exception("Spark coin identification test failed: $e");
    }
  }

  void _updateStatus(TestSuiteStatus newStatus) {
    _status = newStatus;
    _statusController.add(newStatus);
  }

  @override
  Future<void> cleanup() async {
    await _statusController.close();
  }
}